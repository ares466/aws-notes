# S3

## Encryption

Encryption at rest in S3 is configured at the object level. S3 supports both client-side encryption and server-side encryption.

When client-side encryption is used, the objects are encrypted before reaching S3. When server-side encryption is used, the plain-text object is encrypted by the S3 service.

There are three types of server-side encryption that can be used:
- Server-Side Encryption with Customer-Provided Keys ([SSE-C](#sse-c))
- Server-Side Encryption with Amazon S3-Managed Keys ([SSE-S3](#sse-s3))
- Server-Side Encryption with KMS Keys stored in AWS KMS (SSE-KMS)

The S3 `PutObject` operation supports the ability to specify the encryption type in a request header (e.g., *x-amz-server-side-encryption*). If this header is not specified, the objects are not encrypted.

The encryption type can be set as a default on the bucket. If no encryption type is set in the *PutObject* operation, the default type is used.

The default type is *not* enforced, but the encryption type can be enforced via the bucket policy.

### SSE-C

In SSE-C, the customer provides the key and Amazon S3 does the encryption. 

S3 generates a hash of the key and stores it with the object. Subsequent read requests for the object compare the hash of the key sent in the request with the hash stored with the object. If the hashes do not match, the request fails.

After encrypting (*PutObject*) or decrypting (*GetObject*) the object, S3 will discard the key.

Use SSE-C when you need to manage the keys yourself.

### SSE-S3

*x-amz-server-side-encryption*: AES256

When using SSE-S3, Amazon S3 manages encryption and keys.

The S3 service creates a common `root key` that is used to encrypt all data keys.

When an object is uploaded to S3, S3 generates a `data key` specific to that object. The data key is used to encrypt the object. 

The root key is used to encrypt the data key. The ciphertext of the dat key is stored with the S3 object. The plain-text data key is discarded.

This process is called `envelope encryption`.

![S3 SSE-S3](../static/images/s3_sses3.png)

Use SSE-S3 when you have the following requirements:
- Do not need to manage keys
- Do not need role separation (encryption and data are managed by same role)

### SSE-KMS

*x-amz-server-side-encryption*: aws:kms

In this encryption mode, S3 will handle the encryption and KMS/S3 will manage the keys.

When you upload an object with SSE-KMS for the first time, S3 will request a new key from KMS. This is the root key that will be used for S3 encryption.

Customers can specify their own KMS key, which means fine-grained controls and auditing are available on the key used to encrypt S3 data.

As a result, if you don't have access to the KMS key, you cannot read the data in S3.

## Storage Classes

### S3 - Standard

The default storage class, `S3 - Standard`, replicates objects across at least three AZs in the same AWS region to achieve eleven 9s of durability. S3 uses `Content-MD5 Checksums` and `Cyclic Redundancy Checks` (CRCs) to detect and fix any data corruption. Customers are billed a storage fee, a transfer out fee, and a per-request fee.
- The storage fee is charged by GB
- The transfer fee is charged by GB
- The request fee is charged per 1,000 requests

S3 - Standard has a milliseconds first byte latency.

### S3 - Infrequent Access (IA)

`S3 - IA` shares a lot of the same characteristics as S3 - standard including the same durability and replication across at least three AZs.

S3-IA is billable by a storage fee, transfer fee, request fee, and a `retrieval fee`.
- The retrieval fee is charged per GB of data retrieved

Standard-IA has a `minimum duration charge` of 30 days. Objects can be stored for less time, but the minimum billing always applies.

Standard-IA has a `minimum capacity charge` of 128KB per object.

Standard-IA should be used for important long-lived data that is infrequently accessed.

### S3 - One Zone-IA

`S3 - One Zone-IA` shares most of the characteristics as S3-IA, but the data is not replicated across AZs. If the AZ fails, the data is gone.

S3 - One Zone-IA should be used for non-critical long-lived data that is infrequently accessed or data that can easily be replaced (e.g., cache data).

### S3 - Glacier Instant

`S3 - Glacier Instance` is similar to S3 Standard-IA, but has cheaper storage at the cost of a more expensive retrieval and longer billable minimums.

S3 - Glacier Instant has a `minimum duration charge` of 90 days and a `minimum capacity charge` of 128KB per object.

S3 - Glacier Instant should be used for long-lived data that is only accessed once per quarter in which retrieval times must be fast.

### S3 - Glacier Flexible

The `S3 - Glacier Flexible` was renamed from `S3 - Glacier`.

Objects stored in the S3 - Glacier Flexible storage class cannot be publicly accessible.

Objects in Glacier Flexible are retrieved to S3 Standard-IA temporarily in order to be accessed.

This storage classes offers flexible retrieval times that have different costs and SLAs:
- Expedited: 1-5 minutes
- Standard: 3-5 hours
- Bulk: 5-12 hours

The first byte latency for this storage class is minutes to hours.

This low cost storage class should be used for archival data where access is very infrequent (e.g., once per year) and minutes to hours of retrieval time is sufficient.

> To remember Glacier - Flexible, think of objects as being kept in `chilled storage`. These objects must be *thawed* before they become available.

### S3 - Glacier Deep Archive

`S3 - Glacier Deep Archive` is the most cost-effictive storage class at the cost of a high minimum duration and longer retrieval times.

Data in Glacier Deep Archive is retrieved to S3 Standard-IA temporarily for access.
- Standard: 12 hours
- Bulk: 48 hours

The first byte latency for this storage classes is hours to days!

Use this storage class for data that is rarely, if ever, accessed and an hours to days retrieval time is acceptable (e.g., legal or regulated data storage).

### S3 - Intelligent Tiering

`S3 - Intelligent Tiering` is not a storage class, but a storage class manager that can move objects between five different storage tiers:
- Frequent Access
- Infrequent Access
- Archive Instant Access
- Archive Access
- Deep Archive

Intelligent tiering monitors the usage of objects and automatically moves any objects not accessed for 30 days to a low cost infrequent access tier and eventually to archive instant access, archive access, and deep archive tiers.

The archive access and deep archive tiers are optional.

Intelligent Tiering has a monitoring and automation cost per 1,000 objects.

S3 Intelligent Tiering should be used for long-lived data with changing or unknown patterns.

### Comparing Storage Classes

| | Retrieval Time | Minimum Duration Charge | Minimum Capacity Charge | Durability | Replicated AZs | Use Case |
| --- | --- | --- | --- | --- | --- | --- |
| S3 - Standard | Instant | None | None | Eleven 9s | At least 3 AZs | Default | 
| S3 - Infrequent Access (IA) | Instant | 30 Days | 128KB per object | Eleven 9s | At least 3 AZs | Irreplacable data that is infrequently accessed. |
| S3 - One Zone IA | Instant | 30 Days | 128KB per object | Eleven 9s (assuming no AZ failure) | No replication - the data only exists in one AZ | Replaceable data that is infrequently accessed |
| S3 - Glacier Instant | Instant | 90 Days | 128KB per object | Eleven 9s | At least 3 AZs | Long-lived data that is accessed once per quarter in which instant retrieval is required |
| S3 - Glacier Flexible | Minutes to hours | 90 Days | 40KB per object | Eleven 9s | At least 3 AZs | Archived data that is very infrequently accessed (e.g., yearly) and minutes to hour retrieval time is sufficient. |
| S3 - Glacier Deep Archive | Hours to Days | 180 Days | 40 KB per object | Eleven 9s | At least 3 AZs | Archival data that is rarely, if ever, accessed in which hours to days retrieval time is acceptable. |

## S3 Lifecycle Configuration

A `lifecycle configuration` is a set of rules that consist of actions on a bucket or group of objects.

Lifecycle rules support `transition` actions and `expiration` actions.

Lifecycle transitions only flow "down". Transitions cannot be used to move an object to a higher storage tier.

*Caption (below): This chart shows the possible transitions of S3 object lifecycles.*
![S3 Lifecycle Transitions](../static/images/s3_lifecycleconfig.png)
*Note how there are some restrictions on transitions:*
- *Transitions can only flow "down". Transitions cannot be used to move data to a higher storage class.*
- *S3-Intelligent Tiering cannot move data to S3-One Zone-IA.*
- *S3 One Zone-IA cannot move data to S3-Glacier Instance Retrieval.*

Exam Tips:
- There is a `minmum 30 day duration` before you can use a lifecycle rule to transition an object from S3-Standard to S3-Standard-IA or S3-OneZone-IA.
- Be aware that smaller objects can actually cost more when transitioned to S3-Standard-IA, S3-Intellint-Tiering, or S3-OneZone-IA due to the minimum billable size requirements (40KB per object)  of those storage classes.
- A single rule cannot transition to Standard-IA or OneZone-IA and THEN to Glacier classes within 30 days due to the duration requirement of the Standard-IA and OneZone-IA storage classes. To transition to Glacier classes before 30 days, use a separate rule.

## Replication

There are two types of S3 replication:
- Cross Region Replication (CRR)
- Single Region Replication (SRR)

Replication requires an IAM role that will be assumed to perform the replication. The IAM role must allow S3 to assume it in the trust policy, allow read access to the source bucket, and write access to the destination bucket.

If data is being replicated across accounts, the destination bucket policy must explicitly allow write access to the replication IAM role..

Replication can be applied to the entire bucket or a subset of objects.

The default is to write the data to the same storage class as the source bucket, but replication can be configured to write to a lower storage class in the destination bucket.

By default, the object ownership will remain with the source account, but replication can be configured to set the owner to the destination account.

The `Replication Time Control` (RTC) feature can be applied to replication. This will guarantee that data will be replicated within 15 minutes.

Exam tips:
- Replication is `not retroactive`. When replication is enabled on a bucket, no existing objects will be replicated.
- Replication requires `bucket versioning` to be enabled.
- Replication is `one-way`. If you write an object to the destination bucket, it will not be replicated back to the source bucket.
- Replication supports unencrypted and server-side encrypted objects. If `SSE-KMS` is being used, extra configuration is required. Replication does not support `SSE-C`.
- Replication requires that the source bucket owner has permissions to the objects.
- Replication only supports user events. Replication does not support system events like lifecycle transitions. It also does not support replication of objects in the Glacier or Glacier Deep Archive storage classes.
- Replication does not support deletes.

S3 replication can be used for the following use cases:
- Log aggregation (SRR)
- Sync PROD and TEST (SRR)
- Data residency requirements (SRR)
- Global resiliency improvements (CRR)
- Reduce latency (CRR)

## S3 - Cross Account Access

There are three ways to grant access to S3 buckets across AWS accounts.
- [Bucket and Object ACLs](#bucket-and-object-acls)
- [Bucket Policies](#bucket-policies)
- [Assume Role](#assume-role)

### Bucket and Object ACLs

Bucket ACLs enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. It defines which AWS accounts or groups are granted access, and the type of access.

> Bucket ACLs are a legacy concept that is no longer recommended by AWS.

Bucket ACLs cannot be used to manage access within the same AWS account as the bucket.

![S3 Bucket ACL](../static/images/s3_bucketacl.png)

Bucket ACLs grant access based on Cononical Account IDs (a legacy account identifier). The *Account canonical user ID* can be found on the *Security Credentials* console.

By adding a target account's canonical user id to a bucket or object's ACL, you can grant access to that account.

### Bucket Policies

A *bucket policy* is a resource-based policy that you can use to grant permissions to your bucket and the objects in it.

Bucket Policy permissions do not apply to objects owned by other AWS accounts.

```json
{
  "Version": "2012-10-17",
  "Statement": [
      {
          "Effect": "Allow",
          "Principal": {
              "AWS": "arn:aws:iam::1234567890:user/iamadmin"
          },
          "Action": [
              "s3:GetObject",
              "s3:PutObject",
              "s3:PutObjectAcl",
              "s3:ListBucket"
          ],
          "Resource": [
              "arn:aws:s3:::<bucket-name>/*",
              "arn:aws:s3:::<bucket-name>"
          ]
      }
  ]
}
```
*Caption: Bucket policy granting S3 access to the iamadmin user in the 1234567890 account.*

### Assume Role

S3 across-account access can also be setup using IAM roles.

Generally, cross account access using assume role looks like:

- The bucket exists in account A. 
- An IAM role exists in account A that has permissions to the S3 bucket. This IAM role grants trust permissions to another AWS account B.  
- An identity within B can assume the role and operate on the S3 bucket.  

The IAM role must grant `sts:AssumeRole` permissions to an identity in the other AWS account, or delegate access via the account root.

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::1234567890:root"
            },
            "Action": "sts:AssumeRole"
        }
    ]
}
```
*Caption: Trust policy that grants the `sts:AssumeRole` permissions to the 1234567890 account.*

### Object Ownership

When S3 ACLs are enabled, bucket owners can choose the preferred object ownership strategy.

![S3 Object Ownership Setting](../static/images/s3_objectownershipenabled.png).

- **Bucket owner preferred** - if new objects written to the bucket specify the bucket-owner-full-control canned ACL, they are owned by the bucket owner. Otherwise, they ware owned by the object writer.
- **Object writer** - The object writer remains the object owner.

If the assume role strategy is being used for object writers, then the objects will be owned by the bucket account.

### Policy Evaluation

All relevant policies are considered when evaluating access to a bucket or objects including:
- Organizational SCP
- Bucket Policy or ACLs
- IAM Permission Boundaries
- Session Policies
- Identity Policies

Given an *effective policy* (the product of all relevant policy statements), access evaluation will follow these steps:

| Step # | Condition | Result if condition satisfied |
| --- | --- | --- |
| 1 | Check for explicit deny | DENY |
| 2 | Check for explicit allow | ALLOW |
| 3 | Implicit Deny | DENY |

When the request is coming from another AWS account, an explicit ALLOW is required from the bucket account *and* from the other AWS account.

## Presigned URLs

Object owners can share objects with others by creating a `presigned URL`, using their own security credentials, to grant time-limited permission to download the objects.

When you create a presigned URL for your object, you must provide your security credentials and then specify a bucket name, an object key, an HTTP method (GET to download the object), and an expiration date and time. The presigned URLs are valid only for the specified duration.

If you created a presigned URL using a temporary token, then the URL expires when the token expires, even if the URL was created with a later expiration time.

When you access an object using a presigned URL, you are accessing the object using the credentials that were used to generate the URL. 

The permissions of the credentials are validated at access time, not at generation time. This means that if permissions to view the object are revoked from the role that generated the presigned URL, any access of the object through presigned URL will no longer work.

Anyone can create a presigned-URL, even if they do not have access to the object. In this case, the bearer of the URL will not be able to access the object either.

*Caption (below): AWS CLI command for generating a presigned URL.*  

```bash
aws s3 presign s3://animal4lifemedia29384/cat_pic.jpeg --expires-in 180
```  

*Caption (below): Presigned URL for accessing an S3 object. Notice the `X-Amz-Security-Token` query parameter.*
```
https://animal4lifemedia29384.s3.us-east-1.amazonaws.com/cat_pic.jpeg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIA3VOCFM2Z7FTRJF76%2F20220730%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220730T124307Z&X-Amz-Expires=180&X-Amz-SignedHeaders=host&X-Amz-Security-Token=IQoJb3JpZ2luX2VjENX%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIGQQlino4PpMef61n5ZvKRkBFMuvD1umFzLfr8rEuO%2BmAiBdUZICdEidrGupnZnMw5yww8pyBAKoFeMw1eWevxNBOSqMAwguEAAaDDgwMTk1NTQ3MzA3NSIMIrcDu0U%2BSvAiVJ8aKukC3%2Fjbdp1DBZtIifYNj%2BkmvCM4%2FCJDLkpbAP%2BedkXLgsNZ9nzJSDv1oWKEgp0c1O1lmWNTkF%2BfXGZKNl0Aw6kIHgrvBSWnkvgkJZ%2FVeZBJ9V%2F2KkvNXAIKfAN7CZLp6MyN1gauBfj4odaIPYQDSMxhtoW0BhqU8Ich%2BdQB%2F34YQQSH66O4cD5TCGK2IZMeUg8ToiDAJYms68vq4trLuPpTwZdeea7H4O2BmZ%2FjRDuoh004G11qa4dtzZPkpMjA6QlpmDC2hxbepfX9UQ4apPuFIx%2BRp6hFHqY9sVAuR1zOjKZx0a3CW0oYvfqzAptrV%2BnLWkqLmPfCW8DJMKXCkfRXpmIfL9iXsIwNuPUdq3KCahmTBAoZDjy0fciT41LiOscR%2Baa9V11%2B5g1wobOGS2VLSXhPU0rv%2FxdbGaBeQRLHMoUwrhmbLS9Z9jZD5vVmRiYUG302QM%2B40BfwlyWaS%2FNJjPcRe6ipu%2BhjATCczJSXBjq0AkiC4ZCmysfXZSZ5OwZ6MAYuNV%2FZ52k9n3JdxjelKuWtf%2BfyluS1WmHnNBncBqPLklKmgkuXE37H3WYXaA%2FZb4HkNxpON5Q8FRBAJaC5rUYSPTwgBpfzmqGjJfShj2zCFWZo5LkO3EbsAL6zd3bMxKP2Olh%2B5u8WLIpEYHikoQJe9xzGmBX4UAlwvEYpjqT3q0PbCQsf20KzuaMuI7XUFViqjhG4zJHCREmmX0ubYglw8jge%2FdMznlCmKxHPbjm%2FhU4zgQQOEzBU%2BwW2zLL00QjQqA8%2F1BijYhejfmqKJ6LS9cpYBXPDQJkdlJwhJRT%2BI0hHBP7RofjFrtD9IrvQsTsDLfdcYUoRun8OUO8jN9u9gEa2tvLkLDPwhC2RS5JymeDth%2BBUv0pwjNW4KeZYOAz%2Fslm4&X-Amz-Signature=db4c7fe8d9f85264ad03e0d8c7b7fc678d2ec32a518e1e9620253a60d1069aec
```

## S3 Select & Glacier Select

S3 can store objects up to 5 TB in size. Often, data consumers want to retrieve the entire object. For large objects, this takes a lot of time.

If you only need to access part of an object, downloading the object and filtering on the client side still incurs the performance and cost penalty.

S3 and Glacier Select allow you to use SQL-like statements to select part of the object filtered on the server. 

S3 Select supports CSV, JSON, Parquet, and BZIP2 compression for JSON and CSV.

![S3 Select](../static/images/s3_select.png)

## Access Points

`S3 Access Points` simplify management of access to S3 buckets and objects.

Each Access Point has different policies that cantrol who can use the access point and which data is available from that access point. Access Point resource policies support network access controls (e.g., from a specific VPC).

Each S3 Access Point has its own endpoint address.

Access Points can be created via the console or using the AWS CLI:
```bash
`aws s3control create-access-point` --name secretcats --account-id 123456789012 --bucket catpics
```

The bucket policy must grant the same access as the access point, or the bucket policy can `delegate` access by allowing all actions through the access point.

![S3 Access Points](../static/images/s3_accesspoints.png)

## Object Versioning

Objects in S3 are immutable. Without versioning, each object is solely accessed via its key. If you write an object to an S3 bucket with the same key as an existing object, the existing object is overwritten and unrecoverable.

Versioning enables storage of multiple versions of objects within a bucket. Operations that would overwrite objects generate a new version.

In a version-enabled bucket, each object contains a key and an id. These two fields are unique within the bucket. The most recently written version of an object is known as the `latest` version or `current` version. When requesting objects, developers can specify an id to access older versions. If no id is specified, the most recent version is assumed.

In a version-enabled bucket, the delete operation creates a new version of the object called a **delete marker**. The delete marker has the effect of hiding all versions of that object. All versions are still available if accessed by a key and version id within a *GET* operation.

To truly delete a specific version of an object, you must specify the key and version id in the delete command.

Object versioning is a bucket-level operation. By default, versioning is disabled. Once it is enabled on a bucket, it cannot be disabled again, though it can be suspended.

The **MFA Delete** feature is enabled within the versioning configuration of a bucket. When MFA is enabled on a bucket, object versions cannot be deleted, and versioning configuration cannot be changed, without providing a OTP generated from an MFA device. 

## Object Lock

`Object Lock` implements a `WORM` (write once read many) security policy in which individual versions cannot be modified or deleted.

Object Lock requires bucket versioning to be enabled.

A bucket can have default Object Lock settings.

Object Lock can only be enabled on *newly-created* buckets. In order to add object lock to an existing bucket, you must contact AWS support.

There are two types of Object Locks:
- Retention Period
    - Compliance Mode
    - Governance Mode
- Legal Hold

### Retention Period Object Lock

With `S3 Object Lock Retention Period`, you specify the days and years when the retention period ends. 
There are two modes in Retention Period:
- In `Compliance` mode, the retention period cannot be adjusted, deleted, or overwritten (including the account root user).
- In `Governance` mode, special permissions can be granted that allow modifying retention locking sessings. 

Governance mode requires the `s3:BypassGovernanceRetention` permission on a principal in order to modify a retention lock. Likewise, the request must contain the `x-amz-bypass-governance-retention:true` header. This header is defaulted on the AWS console.

### Legal Hold Object Lock

In Object Lock `Legal Hold`, a legal hold flag is set to either true or false on an object version. You cannot specify any kind of retention period.

When a legal hold is enabled, the object version cannot be modified or deleted.

The `s3:PutObjectLegalHold` permission is required to add or remove a legal hold.

![S3 Object Lock](../static/images/s3_objectlock.png)

# Amazon Macie

`Amazon Macie` is a service that discovers, monitors, and protects sensitive data within an S3 data.

Macie will automatically discover sensitive data including PII, PHI, and finance data.

Macie discovers sensitive data in S3 buckets via **Discovery Jobs**. Discovery Jobs result in **findings** in Security Hub, or by publishing events to EventBridge.

Macie uses **data identiers** to discover sensitive data. There are two types of data identifiers:
- **Managed data identifiers** are pre-defined identifiers that use machine learning and patterns to identify common types of sensitive data.
- **Custom data identifiers** are regex-based propietary identifiers. 

Custom identifiers support optional `keywords` that must be in some `proximity` to the regex. The `maximum match distance` setting is used to determine how close keyworks must be to the regex pattern.

Custom identifiers also support `ignore words`. If the regex contains an *ignore word*, the finding is ignored.

Macie uses a multi-account structure using AWS Organizations or by explictly inviting accounts.

![Amazon Macie](../static/images/macie.png)

Macie will generate two types of findings:
- `Policy findings` are generated when settings are discovered that reduce the security of an S3 bucket (e.g., *Policy:IAMUser/S3BlockPublicAccessDisabled*, *Policy:IAMUser/S3BucketEncryptionDisabled*).
- `Sensitive data findings` are generated when sensitive data is discovered in an S3 bucket (e.g., *SensitiveData:S3Object/Credentials*, *SensitiveData:S3Object/CustomIdentifier*).